<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Geometry-Aware Principal Component Analysis for Symmetric Positive Definite Matrices | ACML 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Geometry-Aware Principal Component Analysis for Symmetric Positive Definite Matrices">

  <meta name="citation_author" content="Horev, Inbal">

  <meta name="citation_author" content="Yger, Florian">

  <meta name="citation_author" content="Sugiyama, Masashi">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 7th Asian Conference on Machine Learning">
<meta name="citation_firstpage" content="1">
<meta name="citation_lastpage" content="16">
<meta name="citation_pdf_url" content="Horev15.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Geometry-Aware Principal Component Analysis for Symmetric Positive Definite Matrices</h1>

	<div id="authors">
	
		Inbal Horev,
	
		Florian Yger,
	
		Masashi Sugiyama
	<br />
	</div>
	<div id="info">
		Proceedings of The 7th Asian Conference on Machine Learning,
		pp. 1â€“16, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Symmetric positive definite (SPD) matrices, e.g. covariance matrices, are ubiquitous in machine learning applications. However, because their size grows as <span class="math">\(n^2\)</span> (where <span class="math">\(n\)</span> is the number of variables) their high-dimensionality is a crucial point when working with them. Thus, it is often useful to apply to them dimensionality reduction techniques. Principal component analysis (PCA) is a canonical tool for dimensionality reduction, which for vector data reduces the dimension of the input data while maximizing the preserved variance. Yet, the commonly used, naive extensions of PCA to matrices result in sub-optimal variance retention. Moreover, when applied to SPD matrices, they ignore the geometric structure of the space of SPD matrices, further degrading the performance. In this paper we develop a new Riemannian geometry based formulation of PCA for SPD matrices that i) preserves more data variance by appropriately extending PCA to matrix data, and ii) extends the standard definition from the Euclidean to the Riemannian geometries. We experimentally demonstrate the usefulness of our approach as pre-processing for EEG signals.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="Horev15.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
