---
title: 'Regularized Policy Gradients: Direct Variance Reduction in Policy Gradient
  Estimation'
abstract: 'Policy gradient algorithms are widely used in reinforcement learning problems
  with continuous action spaces, which update the policy parameters along the steepest
  direction of the expected return. However, large variance of policy gradient estimation
  often causes instability of policy update. In this paper, we propose to suppress
  the variance of gradient estimation by directly employing the variance of policy
  gradients as a regularizer. Through experiments, we demonstrate that the proposed
  variance-regularization technique combined with parameter-based exploration and
  baseline subtraction provides more reliable policy updates than non-regularized
  counterparts. '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Zhao15b
month: 0
firstpage: 333
lastpage: 348
page: 333-348
sections: 
author:
- given: Tingting
  family: Zhao
- given: Gang
  family: Niu
- given: Ning
  family: Xie
- given: Jucheng
  family: Yang
- given: Masashi
  family: Sugiyama
date: 2016-02-25
address: Hong Kong
publisher: PMLR
container-title: Asian Conference on Machine Learning
volume: '45'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 2
  - 25
pdf: http://proceedings.mlr.press/v45/Zhao15b.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
