---
title: Geometry-Aware Principal Component Analysis for Symmetric Positive Definite
  Matrices
abstract: Symmetric positive definite (SPD) matrices, e.g. covariance matrices, are
  ubiquitous in machine learning applications. However, because their size grows as
  n^2 (where n is the number of variables) their high-dimensionality is a crucial
  point when working with them. Thus, it is often useful to apply to them dimensionality
  reduction techniques. Principal component analysis (PCA) is a canonical tool for
  dimensionality reduction, which for vector data reduces the dimension of the input
  data while maximizing the preserved variance. Yet, the commonly used, naive extensions
  of PCA to matrices result in sub-optimal variance retention. Moreover, when applied
  to SPD matrices, they ignore the geometric structure of the space of SPD matrices,
  further degrading the performance. In this paper we develop a new Riemannian geometry
  based formulation of PCA for SPD matrices that i) preserves more data variance by
  appropriately extending PCA to matrix data, and ii) extends the standard definition
  from the Euclidean to the Riemannian geometries. We experimentally demonstrate the
  usefulness of our approach as pre-processing for EEG signals.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Horev15
month: 0
firstpage: 1
lastpage: 16
page: 1-16
sections: 
author:
- given: Inbal
  family: Horev
- given: Florian
  family: Yger
- given: Masashi
  family: Sugiyama
date: 2016-02-25
address: Hong Kong
publisher: PMLR
container-title: Asian Conference on Machine Learning
volume: '45'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 2
  - 25
pdf: http://proceedings.mlr.press/v45/Horev15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
