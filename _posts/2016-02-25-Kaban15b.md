---
title: 'A New Look at Nearest Neighbours: Identifying Benign Input Geometries via
  Random Projections'
abstract: 'It is well known that in general, the nearest neighbour rule (NN) has sample
  complexity that is exponential in the input space dimension d when only smoothness
  is assumed on the label posterior function. Here we consider NN on randomly projected  data,
  and we show that, if the input domain has a small "metric size", then the sample
  complexity becomes exponential in the metric entropy integral of the set of normalised
  chords of the input domain. This metric entropy integral measures the complexity
  of the input domain, and can be much smaller than d â€“ for instance in cases when
  the data lies in a  linear or a smooth nonlinear subspace of the ambient space,
  or when it has a sparse representation. We then show that the guarantees we obtain
  for the compressive NN also hold for the dataspace NN in bounded domains; thus the
  random projection takes the role of an analytic tool to identify benign structures
  under which NN learning is possible from a small sample size. Numerical simulations
  on data designed to have intrinsically low complexity confirm our theoretical findings,
  and display a striking agreement in the empirical performances of compressive NN
  and dataspace NN. This suggests that high dimensional data sets that have a low
  complexity underlying structure are well suited for computationally cheap  compressive
  NN learning.   '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Kaban15b
month: 0
firstpage: 65
lastpage: 80
page: 65-80
sections: 
author:
- given: Ata
  family: Kaban
date: 2016-02-25
address: Hong Kong
publisher: PMLR
container-title: Asian Conference on Machine Learning
volume: '45'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 2
  - 25
pdf: http://proceedings.mlr.press/v45/Kaban15b/Kaban15b.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
