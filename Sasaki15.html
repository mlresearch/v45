<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Sufficient Dimension Reduction via Direct Estimation of the Gradients of Logarithmic Conditional Densities | ACML 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Sufficient Dimension Reduction via Direct Estimation
of the Gradients of Logarithmic Conditional Densities">

  <meta name="citation_author" content="Sasaki, Hiroaki">

  <meta name="citation_author" content="Tangkaratt, Voot">

  <meta name="citation_author" content="Sugiyama, Masashi">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 7th Asian Conference on Machine Learning">
<meta name="citation_firstpage" content="33">
<meta name="citation_lastpage" content="48">
<meta name="citation_pdf_url" content="Sasaki15.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Sufficient Dimension Reduction via Direct Estimation of the Gradients of Logarithmic Conditional Densities</h1>

	<div id="authors">
	
		Hiroaki Sasaki,
	
		Voot Tangkaratt,
	
		Masashi Sugiyama
	<br />
	</div>
	<div id="info">
		Proceedings of The 7th Asian Conference on Machine Learning,
		pp. 33â€“48, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Sufficient dimension reduction (SDR) is a framework of supervised linear dimension reduction, and is aimed at finding a low-dimensional orthogonal projection matrix for input data such that the projected input data retains maximal information on output data. A computationally efficient approach employs gradient estimates of the conditional density of the output given input data to find an appropriate projection matrix. However, since the gradients of the conditional densities are typically estimated by a local linear smoother, it does not perform well when the input dimensionality is high. In this paper, we propose a novel estimator of the gradients of logarithmic conditional densities called the <em>least-squares logarithmic conditional density gradients</em> (LSLCG), which fits a gradient model <em>directly</em> to the true gradient without conditional density estimation under the squared loss. Thanks to the simple least-squares formulation, LSLCG gives a closed-form solution that can be computed efficiently. In addition, all the parameters can be automatically determined by cross-validation. Through experiments on a large variety of artificial and benchmark datasets, we demonstrate that the SDR method based on LSLCG outperforms existing SDR methods both in estimation accuracy and computational efficiency.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="Sasaki15.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
