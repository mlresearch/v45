<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Consistency of structured output learning with missing labels | ACML 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Consistency of structured output learning with missing labels">

  <meta name="citation_author" content="Antoniuk, Kostiantyn">

  <meta name="citation_author" content="Franc, Vojtech">

  <meta name="citation_author" content="Hlavac, Vaclav">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 7th Asian Conference on Machine Learning">
<meta name="citation_firstpage" content="81">
<meta name="citation_lastpage" content="95">
<meta name="citation_pdf_url" content="Antoniuk15.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Consistency of structured output learning with missing labels</h1>

	<div id="authors">
	
		Kostiantyn Antoniuk,
	
		Vojtech Franc,
	
		Vaclav Hlavac
	<br />
	</div>
	<div id="info">
		Proceedings of The 7th Asian Conference on Machine Learning,
		pp. 81â€“95, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		In this paper we study statistical consistency of partial losses suitable for learning structured output predictors from examples containing missing labels. We provide sufficient conditions on data generating distribution which admit to prove that the expected risk of the structured predictor learned by minimizing the partial loss converges to the optimal Bayes risk defined by an associated complete loss. We define a concept of surrogate classification calibrated partial losses which are easier to optimize yet their minimization preserves the statistical consistency. We give some concrete examples of surrogate partial losses which are classification calibrated. In particular, we show that the ramp-loss which is in the core of many existing algorithms is classification calibrated.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="Antoniuk15.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
