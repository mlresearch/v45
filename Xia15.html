<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Budgeted Bandit Problems with Continuous Random Costs | ACML 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Budgeted Bandit Problems with Continuous Random Costs">

  <meta name="citation_author" content="Xia, Yingce">

  <meta name="citation_author" content="Ding, Wenkui">

  <meta name="citation_author" content="Zhang, Xu-Dong">

  <meta name="citation_author" content="Yu, Nenghai">

  <meta name="citation_author" content="Qin, Tao">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 7th Asian Conference on Machine Learning">
<meta name="citation_firstpage" content="317">
<meta name="citation_lastpage" content="332">
<meta name="citation_pdf_url" content="Xia15.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Budgeted Bandit Problems with Continuous Random Costs</h1>

	<div id="authors">
	
		Yingce Xia,
	
		Wenkui Ding,
	
		Xu-Dong Zhang,
	
		Nenghai Yu,
	
		Tao Qin
	<br />
	</div>
	<div id="info">
		Proceedings of The 7th Asian Conference on Machine Learning,
		pp. 317â€“332, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We study the budgeted bandit problem, where each arm is associated with both a reward and a cost. In a budgeted bandit problem, the objective is to design an arm pulling algorithm in order to maximize the total reward before the budget runs out. In this work, we study both multi-armed bandits and linear bandits, and focus on the setting with continuous random costs. We propose an upper confidence bound based algorithm for multi-armed bandits and a confidence ball based algorithm for linear bandits, and prove logarithmic regret bounds for both algorithms. We conduct simulations on the proposed algorithms, which verify the effectiveness of our proposed algorithms.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="Xia15.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
